{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da7fe62-a8f0-4da9-bb04-1dbd91337ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Comienzo capa Silver (Raw Bronce - Silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9e722f-4e70-4ff7-b176-46e1ac43e651",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean and Split Global Superstore Sales Data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, split, col\n",
    "\n",
    "df_test = spark.read.csv(\"/mnt/datalake/globalsalesmarketmedallon/global_sales_raw/global_superstore_data.csv\")\n",
    "df_test.show(5, truncate=False)\n",
    "\n",
    "# Limpieza: quitar comillas y convertir \"\\t\" en tab real\n",
    "df_clean = df_test.withColumn(\"cleaned\", regexp_replace(\"_c0\", '\"', ''))\n",
    "df_clean = df_clean.withColumn(\"cleaned\", regexp_replace(\"cleaned\", \"\\\\\\\\t\", \"\\t\"))\n",
    "\n",
    "df_split = df_clean.withColumn(\"columns\", split(col(\"cleaned\"), \"\\t\"))\n",
    "\n",
    "df_split.selectExpr(\"size(columns) as num_cols\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd08371-f26f-423e-bf44-aa8c4d7e7c0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating DataFrame with Renamed Columns"
    }
   },
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    \"Category\", \"City\", \"Country\", \"Customer_ID\", \"Customer_Name\", \"Discount\", \"Market\", \"Aasdas\"\n",
    "    \"564mg54o\", \"Order_Date\", \"Order_ID\", \"Order_Priority\", \"Product_ID\", \"Product_Name\",\n",
    "    \"Profit\", \"Quantity\", \"Region\", \"Row_ID\", \"Sales\", \"Segment\", \"Ship_Date\", \"Ship_Mode\",\n",
    "    \"Shipping_Cost\", \"State\", \"Sub_Category\", \"Year\", \"Market2\", \"Week_Num\"\n",
    "]\n",
    "\n",
    "# Convertir la columna \"columns\" (array) en columnas individuales\n",
    "df_final = df_split.select([col(\"columns\")[i].alias(name) for i, name in enumerate(column_names)])\n",
    "\n",
    "df_final.show(5, truncate=False)\n",
    "\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c19b6e8b-79eb-4c02-92aa-7701cd4e077b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Final Bronze DataFrame to CSV and Delta Format"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# save to silver\n",
    "df_final.coalesce(1).write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/mnt/datalake/globalsalesmarketmedallon/global_sales_bronce/csv\")\n",
    "\n",
    "df_final.coalesce(1).write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/mnt/datalake/globalsalesmarketmedallon/global_sales_bronce/delta_tables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6148ac71-66d2-4cc6-9315-47840435c867",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Global Sales Data from Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_plata = spark.read.format(\"delta\").load(\"/mnt/datalake/globalsalesmarketmedallon/global_sales_bronce/delta_tables\")\n",
    "display(df_plata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d6c099b-a8d7-4ff8-b69c-6c66d990bbd8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transform and Clean Silver Layer DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Limpieza y Transformacion en capa plata (Silver)\n",
    "\n",
    "new_header = df_plata.limit(1).collect()[0].asDict().values()\n",
    "columns = list(new_header)\n",
    "\n",
    "# Reemplazar espacios por guiones bajos y cambiar 'Customer ID' a 'Customer_ID'\n",
    "columns = [col.replace(\" \", \"_\") if \" \" in col else col for col in columns]\n",
    "columns = [\"Customer_ID\" if col == \"Customer_ID\" else col for col in columns]\n",
    "columns = [col.replace(\";;;;;.\", \"\") if col.endswith(\";;;;;.\") else col for col in columns]\n",
    "\n",
    "df_clean = df_plata.rdd.zipWithIndex().filter(lambda x: x[1] > 1).map(lambda x: x[0]).toDF(columns)\n",
    "df_clean = df_plata.withColumn(\"Week_Num\", regexp_replace(col(\"Week_Num\"), \";+\", \"\").cast(\"string\"))\n",
    "\n",
    "# Eliminacion de header duplicado en 1er fila , al seleccionar solo Category se borra la fila completa\n",
    "df_clean = df_plata.filter(df_plata[\"Category\"] != \"Category\")\n",
    "\n",
    "# Vuelvo a limpiar columna Week_Num quitandole los ;\n",
    "df_clean = df_clean.withColumn(\"Week_Num\", regexp_replace(col(\"Week_Num\"), \";+\", \"\"))\n",
    "\n",
    "display(df_clean)\n",
    "\n",
    "# df_clean.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/datalake/globalsalesmarketmedallon/global_sales_plata/delta_tables\")\n",
    "# df_clean.write.format(\"csv\").mode(\"overwrite\").save(\"/mnt/datalake/globalsalesmarketmedallon/global_sales_plata/csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b5b1501-674c-4e0e-b562-14aa54dbd6c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop Unnecessary Columns and Rename Market Column"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_clean = df_clean.drop(\"AasdasRecord_Count\")\n",
    "df_clean = df_clean.withColumnRenamed(\"Market\", \"Continent\")\n",
    "df_clean = df_clean.drop(\"Market2\")\n",
    "df_clean = df_clean.drop(\"Aasdas564mg54o\")\n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56bd0de-0c76-4671-b1d6-28d3aaaf1f06",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cast and Format Columns in DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "# casteo de columnas a diferentes tipos de datos\n",
    "\n",
    "df_clean = df_clean.withColumn(\"Discount\", col(\"Discount\").cast(\"int\"))\n",
    "df_clean = df_clean.withColumn(\"Order_Date\", date_format(col(\"Order_Date\").cast(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_clean = df_clean.withColumn(\"Profit\", col(\"Profit\").cast(\"double\"))\n",
    "df_clean = df_clean.withColumn(\"Quantity\", col(\"Quantity\").cast(\"int\"))\n",
    "df_clean = df_clean.withColumn(\"Ship_Date\", date_format(col(\"Order_Date\").cast(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_clean = df_clean.withColumn(\"Shipping_Cost\", col(\"Shipping_Cost\").cast(\"double\"))\n",
    "df_clean = df_clean.withColumn(\"Sales\", col(\"Sales\").cast(\"int\"))\n",
    "df_clean = df_clean.withColumn(\"Year\", col(\"Year\").cast(\"int\"))\n",
    "\n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b032958b-6383-4f99-b3a0-d70e0af728a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Standardize Case and Trim Whitespace in Key Columns"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper, trim\n",
    "# Estandarizo columna Order_Priority y Ship_Mode en MAYUSCULA y trim para espacios en blanco\n",
    "df_clean = df_clean.withColumn(\"Order_Priority\", upper(trim(col(\"Order_Priority\")))) \\\n",
    "       .withColumn(\"Ship_Mode\", upper(trim(col(\"Ship_Mode\")))) \\\n",
    "       .withColumn(\"Sub_Category\", trim(col(\"Sub_Category\"))) \n",
    "\n",
    "display(df_clean)\n",
    "\n",
    "\n",
    "## Mal manejo de los viajes desde el csv ingestado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60a95226-19fa-4091-8fb6-da94daeb2cbe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add Year, Month, Week, and Shipping Days Columns"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import year, month, weekofyear, datediff\n",
    "\n",
    "# creamos nuevas columnas para el a√±o, mes, semana y dias de entrega del pedido para mas informacion de analisis \n",
    "df_clean = df_clean.withColumn(\"Order_Year\", year(\"Order_Date\")) \\\n",
    "       .withColumn(\"Order_Month\", month(\"Order_Date\")) \\\n",
    "       .withColumn(\"Order_Week\", weekofyear(\"Order_Date\")) \\\n",
    "       .withColumn(\"Shipping_Days\", datediff(\"Ship_Date\", \"Order_Date\")) \\\n",
    "       .withColumn(\"Profit_Margin\", col(\"Profit\") / col(\"Sales\"))\n",
    "\n",
    "# Check de envio con fecha posterior a la orden (Fallo por misma fecha y hora), se podria borrar la columna \"Shipping_Days\"\n",
    "df_clean = df_clean.filter(col(\"Ship_Date\") >= col(\"Order_Date\"))\n",
    "\n",
    "display(df_clean)\n",
    "\n",
    "df_clean.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/datalake/globalsalesmarketmedallon/global_sales_plata/delta_tables\")\n",
    "df_clean.write.format(\"csv\").mode(\"overwrite\").save(\"/mnt/datalake/globalsalesmarketmedallon/global_sales_plata/csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00bda4b-b369-46ce-8b81-cbbc288eb30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fin capa plata (Silver)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL_global_sales_plata",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
